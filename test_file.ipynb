{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2517c8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in c:\\users\\srinivas kothapalli\\anaconda3\\lib\\site-packages (7.4.3)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\srinivas kothapalli\\anaconda3\\lib\\site-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\srinivas kothapalli\\appdata\\roaming\\python\\python311\\site-packages (from pytest) (23.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\srinivas kothapalli\\anaconda3\\lib\\site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\srinivas kothapalli\\appdata\\roaming\\python\\python311\\site-packages (from pytest) (0.4.6)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tests.pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\srinivas kothapalli\\Desktop\\my_project_sadanand\\DLT\\tests\\load\\pipeline\\Untitled.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/srinivas%20kothapalli/Desktop/my_project_sadanand/DLT/tests/load/pipeline/Untitled.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdlt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextract\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msource\u001b[39;00m \u001b[39mimport\u001b[39;00m DltResource\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/srinivas%20kothapalli/Desktop/my_project_sadanand/DLT/tests/load/pipeline/Untitled.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdlt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msources\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhelpers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransform\u001b[39;00m \u001b[39mimport\u001b[39;00m skip_first, take_first\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/srinivas%20kothapalli/Desktop/my_project_sadanand/DLT/tests/load/pipeline/Untitled.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m assert_load_info\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/srinivas%20kothapalli/Desktop/my_project_sadanand/DLT/tests/load/pipeline/Untitled.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_table_counts, select_data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/srinivas%20kothapalli/Desktop/my_project_sadanand/DLT/tests/load/pipeline/Untitled.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m destinations_configs, DestinationTestConfiguration\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tests.pipeline'"
     ]
    }
   ],
   "source": [
    "\n",
    "from copy import copy\n",
    "! pip install pytest\n",
    "import pytest\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "from typing import List\n",
    "import pytest\n",
    "import yaml\n",
    "\n",
    "import dlt\n",
    "\n",
    "\n",
    "from dlt.common import json, pendulum\n",
    "from dlt.common.configuration.container import Container\n",
    "from dlt.common.pipeline import StateInjectableContext\n",
    "from dlt.common.typing import AnyFun, StrAny\n",
    "from dlt.common.utils import digest128\n",
    "from dlt.extract.source import DltResource\n",
    "from dlt.sources.helpers.transform import skip_first, take_first\n",
    "\n",
    "\n",
    "from tests.pipeline.utils import assert_load_info\n",
    "from tests.load.pipeline.utils import load_table_counts, select_data\n",
    "from tests.load.pipeline.utils import destinations_configs, DestinationTestConfiguration\n",
    "\n",
    "# uncomment add motherduck tests\n",
    "# NOTE: the tests are passing but we disable them due to frequent ATTACH DATABASE timeouts\n",
    "# ACTIVE_DESTINATIONS += [\"motherduck\"]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_merge_on_keys_in_schema(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"eth_2\", full_refresh=True)\n",
    "\n",
    "    with open(\"tests/common/cases/schemas/eth/ethereum_schema_v5.yml\", \"r\", encoding=\"utf-8\") as f:\n",
    "        schema = dlt.Schema.from_dict(yaml.safe_load(f))\n",
    "\n",
    "    with open(\"tests/normalize/cases/ethereum.blocks.9c1d9b504ea240a482b007788d5cd61c_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # take only the first block. the first block does not have uncles so this table should not be created and merged\n",
    "    info = p.run(data[:1], table_name=\"blocks\", write_disposition=\"merge\", schema=schema)\n",
    "    assert_load_info(info)\n",
    "    eth_1_counts = load_table_counts(p, \"blocks\")\n",
    "    # we load a single block\n",
    "    assert eth_1_counts[\"blocks\"] == 1\n",
    "    # check root key propagation\n",
    "    assert p.default_schema.tables[\"blocks__transactions\"][\"columns\"][\"_dlt_root_id\"][\"root_key\"] is True\n",
    "    # now we load the whole dataset. blocks should be created which adds columns to blocks\n",
    "    # if the table would be created before the whole load would fail because new columns have hints\n",
    "    info = p.run(data, table_name=\"blocks\", write_disposition=\"merge\", schema=schema)\n",
    "    eth_2_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    # we have 2 blocks in dataset\n",
    "    assert eth_2_counts[\"blocks\"] == 2 if destination_config.supports_merge else 3\n",
    "    # make sure we have same record after merging full dataset again\n",
    "    info = p.run(data, table_name=\"blocks\", write_disposition=\"merge\", schema=schema)\n",
    "    assert_load_info(info)\n",
    "    # for non merge destinations we just check that the run passes\n",
    "    if not destination_config.supports_merge:\n",
    "        return\n",
    "    eth_3_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert eth_2_counts == eth_3_counts\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_merge_on_ad_hoc_primary_key(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"github_1\", full_refresh=True)\n",
    "\n",
    "    with open(\"tests/normalize/cases/github.issues.load_page_5_duck.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # note: NodeId will be normalized to \"node_id\" which exists in the schema\n",
    "    info = p.run(data[:17], table_name=\"issues\", write_disposition=\"merge\", primary_key=\"NodeId\")\n",
    "    assert_load_info(info)\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    # 17 issues\n",
    "    assert github_1_counts[\"issues\"] == 17\n",
    "    # primary key set on issues\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"node_id\"][\"primary_key\"] is True\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"node_id\"][\"data_type\"] == \"text\"\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"node_id\"][\"nullable\"] is False\n",
    "\n",
    "    info = p.run(data[5:], table_name=\"issues\", write_disposition=\"merge\", primary_key=\"node_id\")\n",
    "    assert_load_info(info)\n",
    "    # for non merge destinations we just check that the run passes\n",
    "    if not destination_config.supports_merge:\n",
    "        return\n",
    "    github_2_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    # 100 issues total\n",
    "    assert github_2_counts[\"issues\"] == 100\n",
    "    # still 100 after the reload\n",
    "\n",
    "\n",
    "@dlt.source(root_key=True)\n",
    "def github():\n",
    "\n",
    "    @dlt.resource(table_name=\"issues\", write_disposition=\"merge\", primary_key=\"id\", merge_key=(\"node_id\", \"url\"))\n",
    "    def load_issues():\n",
    "        with open(\"tests/normalize/cases/github.issues.load_page_5_duck.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            yield from json.load(f)\n",
    "\n",
    "    return load_issues\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_merge_source_compound_keys_and_changes(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"github_3\", full_refresh=True)\n",
    "\n",
    "    info = p.run(github())\n",
    "    assert_load_info(info)\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    # 100 issues total\n",
    "    assert github_1_counts[\"issues\"] == 100\n",
    "    # check keys created\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"node_id\"].items() > {\"merge_key\": True, \"data_type\": \"text\", \"nullable\": False}.items()\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"url\"].items() > {\"merge_key\": True, \"data_type\": \"text\", \"nullable\": False}.items()\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"id\"].items() > {\"primary_key\": True, \"data_type\": \"bigint\", \"nullable\": False}.items()\n",
    "\n",
    "    # append load_issues resource\n",
    "    info = p.run(github().load_issues, write_disposition=\"append\")\n",
    "    assert_load_info(info)\n",
    "    assert p.default_schema.tables[\"issues\"][\"write_disposition\"] == \"append\"\n",
    "    # the counts of all tables must be double\n",
    "    github_2_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert {k:v*2 for k, v in github_1_counts.items()} == github_2_counts\n",
    "\n",
    "    # now replace all resources\n",
    "    info = p.run(github(), write_disposition=\"replace\" )\n",
    "    assert_load_info(info)\n",
    "    assert p.default_schema.tables[\"issues\"][\"write_disposition\"] == \"replace\"\n",
    "    # assert p.default_schema.tables[\"issues__labels\"][\"write_disposition\"] == \"replace\"\n",
    "    # the counts of all tables must be double\n",
    "    github_3_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_1_counts == github_3_counts\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_merge_no_child_tables(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"github_3\", full_refresh=True)\n",
    "    github_data = github()\n",
    "    assert github_data.max_table_nesting is None\n",
    "    assert github_data.root_key is True\n",
    "    # set max nesting to 0 so no child tables are generated\n",
    "    github_data.max_table_nesting = 0\n",
    "    assert github_data.max_table_nesting == 0\n",
    "    github_data.root_key = False\n",
    "    assert github_data.root_key is False\n",
    "\n",
    "    # take only first 15 elements\n",
    "    github_data.load_issues.add_filter(take_first(15))\n",
    "    info = p.run(github_data)\n",
    "    assert len(p.default_schema.data_tables()) == 1\n",
    "    assert \"issues\" in p.default_schema.tables\n",
    "    assert_load_info(info)\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_1_counts[\"issues\"] == 15\n",
    "\n",
    "    # load all\n",
    "    github_data = github()\n",
    "    github_data.max_table_nesting = 0\n",
    "    info = p.run(github_data)\n",
    "    assert_load_info(info)\n",
    "    github_2_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    # 100 issues total, or 115 if merge is not supported\n",
    "    assert github_2_counts[\"issues\"] == 100 if destination_config.supports_merge else 115\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_merge_no_merge_keys(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"github_3\", full_refresh=True)\n",
    "    github_data = github()\n",
    "    # remove all keys\n",
    "    github_data.load_issues.apply_hints(merge_key=(), primary_key=())\n",
    "    # skip first 45 rows\n",
    "    github_data.load_issues.add_filter(skip_first(45))\n",
    "    info = p.run(github_data)\n",
    "    assert_load_info(info)\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_1_counts[\"issues\"] == 100 - 45\n",
    "\n",
    "    # take first 10 rows.\n",
    "    github_data = github()\n",
    "    # remove all keys\n",
    "    github_data.load_issues.apply_hints(merge_key=(), primary_key=())\n",
    "    # skip first 45 rows\n",
    "    github_data.load_issues.add_filter(take_first(10))\n",
    "    info = p.run(github_data)\n",
    "    assert_load_info(info)\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    # only ten rows remains. merge falls back to replace when no keys are specified\n",
    "    assert github_1_counts[\"issues\"] == 10 if destination_config.supports_merge else 100 - 45\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_merge_keys_non_existing_columns(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"github_3\", full_refresh=True)\n",
    "    github_data = github()\n",
    "    # set keys names that do not exist in the data\n",
    "    github_data.load_issues.apply_hints(merge_key=(\"mA1\", \"Ma2\"), primary_key=(\"123-x\", ))\n",
    "    # skip first 45 rows\n",
    "    github_data.load_issues.add_filter(skip_first(45))\n",
    "    info = p.run(github_data)\n",
    "    assert_load_info(info)\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_1_counts[\"issues\"] == 100 - 45\n",
    "    assert p.default_schema.tables[\"issues\"][\"columns\"][\"m_a1\"].items() > {\"merge_key\": True, \"nullable\": False}.items()\n",
    "\n",
    "    # for non merge destinations we just check that the run passes\n",
    "    if not destination_config.supports_merge:\n",
    "        return\n",
    "\n",
    "    # all the keys are invalid so the merge falls back to replace\n",
    "    github_data = github()\n",
    "    github_data.load_issues.apply_hints(merge_key=(\"mA1\", \"Ma2\"), primary_key=(\"123-x\", ))\n",
    "    github_data.load_issues.add_filter(take_first(1))\n",
    "    info = p.run(github_data)\n",
    "    assert_load_info(info)\n",
    "    github_2_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_2_counts[\"issues\"] == 1\n",
    "    with p._sql_job_client(p.default_schema) as job_c:\n",
    "        _, table_schema = job_c.get_storage_table(\"issues\")\n",
    "        assert \"url\" in table_schema\n",
    "        assert \"m_a1\" not in table_schema  # unbound columns were not created\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True, subset=[\"duckdb\", \"snowflake\", \"bigquery\"]), ids=lambda x: x.name)\n",
    "def test_pipeline_load_parquet(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"github_3\", full_refresh=True)\n",
    "    github_data = github()\n",
    "    # generate some complex types\n",
    "    github_data.max_table_nesting = 2\n",
    "    github_data_copy = github()\n",
    "    github_data_copy.max_table_nesting = 2\n",
    "    info = p.run([github_data, github_data_copy], loader_file_format=\"parquet\", write_disposition=\"merge\")\n",
    "    assert_load_info(info)\n",
    "    # make sure it was parquet or sql transforms\n",
    "    files = p.get_load_package_info(p.list_completed_load_packages()[0]).jobs[\"completed_jobs\"]\n",
    "    assert all(f.job_file_info.file_format in [\"parquet\", \"sql\"] for f in files)\n",
    "\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_1_counts[\"issues\"] == 100\n",
    "\n",
    "    # now retry with replace\n",
    "    github_data = github()\n",
    "    # generate some complex types\n",
    "    github_data.max_table_nesting = 2\n",
    "    info = p.run(github_data, loader_file_format=\"parquet\", write_disposition=\"replace\")\n",
    "    assert_load_info(info)\n",
    "    # make sure it was parquet or sql inserts\n",
    "    files = p.get_load_package_info(p.list_completed_load_packages()[1]).jobs[\"completed_jobs\"]\n",
    "    assert all(f.job_file_info.file_format in [\"parquet\"] for f in files)\n",
    "\n",
    "    github_1_counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables()])\n",
    "    assert github_1_counts[\"issues\"] == 100\n",
    "\n",
    "\n",
    "\n",
    "@dlt.transformer(name=\"github_repo_events\", primary_key=\"id\", write_disposition=\"merge\", table_name=lambda i: i['type'])\n",
    "def github_repo_events(page: List[StrAny], last_created_at = dlt.sources.incremental(\"created_at\", \"1970-01-01T00:00:00Z\")):\n",
    "    \"\"\"A transformer taking a stream of github events and dispatching them to tables named by event type. Deduplicates be 'id'. Loads incrementally by 'created_at' \"\"\"\n",
    "    yield page\n",
    "\n",
    "\n",
    "@dlt.transformer(name=\"github_repo_events\", primary_key=\"id\", write_disposition=\"merge\")\n",
    "def github_repo_events_table_meta(page: List[StrAny], last_created_at = dlt.sources.incremental(\"created_at\", \"1970-01-01T00:00:00Z\")):\n",
    "    \"\"\"A transformer taking a stream of github events and dispatching them to tables using table meta. Deduplicates be 'id'. Loads incrementally by 'created_at' \"\"\"\n",
    "    yield from [dlt.mark.with_table_name(p, p['type']) for p in page]\n",
    "\n",
    "\n",
    "@dlt.resource\n",
    "def _get_shuffled_events(shuffle: bool = dlt.secrets.value):\n",
    "    with open(\"tests/normalize/cases/github.events.load_page_1_duck.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        issues = json.load(f)\n",
    "        # random order\n",
    "        if shuffle:\n",
    "            random.shuffle(issues)\n",
    "        yield issues\n",
    "\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "@pytest.mark.parametrize(\"github_resource\",[github_repo_events, github_repo_events_table_meta])\n",
    "def test_merge_with_dispatch_and_incremental(destination_config: DestinationTestConfiguration, github_resource: DltResource) -> None:\n",
    "    newest_issues = list(sorted(_get_shuffled_events(True), key = lambda x: x[\"created_at\"], reverse=True))\n",
    "    newest_issue = newest_issues[0]\n",
    "\n",
    "    @dlt.resource\n",
    "    def _new_event(node_id):\n",
    "        new_i = copy(newest_issue)\n",
    "        new_i[\"id\"] = str(random.randint(0, 2^32))\n",
    "        new_i[\"created_at\"] = pendulum.now().isoformat()\n",
    "        new_i[\"node_id\"] = node_id\n",
    "        # yield pages\n",
    "        yield [new_i]\n",
    "\n",
    "    @dlt.resource\n",
    "    def _updated_event(node_id):\n",
    "        new_i = copy(newest_issue)\n",
    "        new_i[\"created_at\"] = pendulum.now().isoformat()\n",
    "        new_i[\"node_id\"] = node_id\n",
    "        # yield pages\n",
    "        yield [new_i]\n",
    "\n",
    "    # inject state that we can inspect and will be shared across calls\n",
    "    with Container().injectable_context(StateInjectableContext(state={})):\n",
    "        assert len(list(_get_shuffled_events(True) | github_resource)) == 100\n",
    "        incremental_state = github_resource.state\n",
    "        assert incremental_state[\"incremental\"][\"created_at\"][\"last_value\"] == newest_issue[\"created_at\"]\n",
    "        assert incremental_state[\"incremental\"][\"created_at\"][\"unique_hashes\"] == [digest128(f'\"{newest_issue[\"id\"]}\"')]\n",
    "        # subsequent load will skip all elements\n",
    "        assert len(list(_get_shuffled_events(True) | github_resource)) == 0\n",
    "        # add one more issue\n",
    "        assert len(list(_new_event(\"new_node\") | github_resource)) == 1\n",
    "        assert incremental_state[\"incremental\"][\"created_at\"][\"last_value\"] > newest_issue[\"created_at\"]\n",
    "        assert incremental_state[\"incremental\"][\"created_at\"][\"unique_hashes\"] != [digest128(str(newest_issue[\"id\"]))]\n",
    "\n",
    "    # load to destination\n",
    "    p = destination_config.setup_pipeline(\"github_3\", full_refresh=True)\n",
    "    info = p.run(_get_shuffled_events(True) | github_resource)\n",
    "    assert_load_info(info)\n",
    "    # get top tables\n",
    "    counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables() if t.get(\"parent\") is None])\n",
    "    # total number of events in all top tables == 100\n",
    "    assert sum(counts.values()) == 100\n",
    "    # this should skip all events due to incremental load\n",
    "    info = p.run(_get_shuffled_events(True) | github_resource)\n",
    "    # no packages were loaded\n",
    "    assert len(info.loads_ids) == 0\n",
    "\n",
    "    # load one more event with a new id\n",
    "    info = p.run(_new_event(\"new_node\") | github_resource)\n",
    "    assert_load_info(info)\n",
    "    counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables() if t.get(\"parent\") is None])\n",
    "    assert sum(counts.values()) == 101\n",
    "    # all the columns have primary keys and merge disposition derived from resource\n",
    "    for table in  p.default_schema.data_tables():\n",
    "        if table.get(\"parent\") is None:\n",
    "            assert table[\"write_disposition\"] == \"merge\"\n",
    "            assert table[\"columns\"][\"id\"][\"primary_key\"] is True\n",
    "\n",
    "    # load updated event\n",
    "    info = p.run(_updated_event(\"new_node_X\") | github_resource)\n",
    "    assert_load_info(info)\n",
    "    # still 101\n",
    "    counts = load_table_counts(p, *[t[\"name\"] for t in p.default_schema.data_tables() if t.get(\"parent\") is None])\n",
    "    assert sum(counts.values()) == 101 if destination_config.supports_merge else 102\n",
    "    # for non merge destinations we just check that the run passes\n",
    "    if not destination_config.supports_merge:\n",
    "        return\n",
    "    # but we have it updated\n",
    "    with p.sql_client() as c:\n",
    "        qual_name = c.make_qualified_table_name(\"watch_event\")\n",
    "        with c.execute_query(f\"SELECT node_id FROM {qual_name} WHERE node_id = 'new_node_X'\") as q:\n",
    "            assert len(list(q.fetchall())) == 1\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_deduplicate_single_load(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"abstract\", full_refresh=True)\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", primary_key=\"id\")\n",
    "    def duplicates():\n",
    "        yield [{\"id\": 1, \"name\": \"row1\", \"child\": [1, 2, 3]}, {\"id\": 1, \"name\": \"row2\", \"child\": [4, 5, 6]}]\n",
    "\n",
    "    info = p.run(duplicates())\n",
    "    assert_load_info(info)\n",
    "    counts = load_table_counts(p, \"duplicates\", \"duplicates__child\")\n",
    "    assert counts[\"duplicates\"] == 1 if destination_config.supports_merge else 2\n",
    "    assert counts[\"duplicates__child\"] == 3 if destination_config.supports_merge else 6\n",
    "    qual_name = p.sql_client().make_qualified_table_name(\"duplicates\")\n",
    "    select_data(p, f\"SELECT * FROM {qual_name}\")[0]\n",
    "\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", primary_key=(\"id\", \"subkey\"))\n",
    "    def duplicates_no_child():\n",
    "        yield [{\"id\": 1, \"subkey\": \"AX\", \"name\": \"row1\"}, {\"id\": 1, \"subkey\": \"AX\", \"name\": \"row2\"}]\n",
    "\n",
    "    info = p.run(duplicates_no_child())\n",
    "    assert_load_info(info)\n",
    "    counts = load_table_counts(p, \"duplicates_no_child\")\n",
    "    assert counts[\"duplicates_no_child\"] == 1 if destination_config.supports_merge else 2\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"destination_config\", destinations_configs(default_sql_configs=True), ids=lambda x: x.name)\n",
    "def test_no_deduplicate_only_merge_key(destination_config: DestinationTestConfiguration) -> None:\n",
    "    p = destination_config.setup_pipeline(\"abstract\", full_refresh=True)\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", merge_key=\"id\")\n",
    "    def duplicates():\n",
    "        yield [{\"id\": 1, \"name\": \"row1\", \"child\": [1, 2, 3]}, {\"id\": 1, \"name\": \"row2\", \"child\": [4, 5, 6]}]\n",
    "\n",
    "    info = p.run(duplicates())\n",
    "    assert_load_info(info)\n",
    "    counts = load_table_counts(p, \"duplicates\", \"duplicates__child\")\n",
    "    assert counts[\"duplicates\"] == 2\n",
    "    assert counts[\"duplicates__child\"] == 6\n",
    "\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", merge_key=(\"id\", \"subkey\"))\n",
    "    def duplicates_no_child():\n",
    "        yield [{\"id\": 1, \"subkey\": \"AX\", \"name\": \"row1\"}, {\"id\": 1, \"subkey\": \"AX\", \"name\": \"row2\"}]\n",
    "\n",
    "    info = p.run(duplicates_no_child())\n",
    "    assert_load_info(info)\n",
    "    counts = load_table_counts(p, \"duplicates_no_child\")\n",
    "    assert counts[\"duplicates_no_child\"] == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\srinivas kothapalli\\Desktop\\my_project_sadanand\\DLT\\tests\\load\\pipeline\\Untitled.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/srinivas%20kothapalli/Desktop/my_project_sadanand/DLT/tests/load/pipeline/Untitled.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m#.utils import assert_load_info\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pipeline'"
     ]
    }
   ],
   "source": [
    "from tests.pipeline import *#.utils import assert_load_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlt.common import json, pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
